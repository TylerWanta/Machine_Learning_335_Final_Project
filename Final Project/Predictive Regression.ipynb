{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_part1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>season</th>\n",
       "      <th>age</th>\n",
       "      <th>tripletsOfMonths</th>\n",
       "      <th>commonRed</th>\n",
       "      <th>commonGreen</th>\n",
       "      <th>commonBlue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44441</th>\n",
       "      <td>44441</td>\n",
       "      <td>17036</td>\n",
       "      <td>Men</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>241</td>\n",
       "      <td>242</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44442</th>\n",
       "      <td>44442</td>\n",
       "      <td>6461</td>\n",
       "      <td>Men</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>223</td>\n",
       "      <td>220</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44443</th>\n",
       "      <td>44443</td>\n",
       "      <td>18842</td>\n",
       "      <td>Men</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>191</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44444</th>\n",
       "      <td>44444</td>\n",
       "      <td>46694</td>\n",
       "      <td>Women</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44445</th>\n",
       "      <td>44445</td>\n",
       "      <td>51623</td>\n",
       "      <td>Women</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44446 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     id gender  season  age  tripletsOfMonths  commonRed  \\\n",
       "0               0  15970    Men    Fall    2                 4        254   \n",
       "1               1  39386    Men  Summer    2                 3         40   \n",
       "2               2  59263  Women  Winter    2                 1        234   \n",
       "3               3  21379    Men    Fall    2                 4         50   \n",
       "4               4  53759    Men  Summer    2                 3          0   \n",
       "...           ...    ...    ...     ...  ...               ...        ...   \n",
       "44441       44441  17036    Men  Summer    2                 3        241   \n",
       "44442       44442   6461    Men  Summer    2                 3        223   \n",
       "44443       44443  18842    Men    Fall    2                 4        144   \n",
       "44444       44444  46694  Women  Spring    2                 2        253   \n",
       "44445       44445  51623  Women  Winter    2                 1        216   \n",
       "\n",
       "       commonGreen  commonBlue  \n",
       "0              254          90  \n",
       "1               53          59  \n",
       "2              234         234  \n",
       "3               50          52  \n",
       "4                0           0  \n",
       "...            ...         ...  \n",
       "44441          242         234  \n",
       "44442          220         213  \n",
       "44443          191         221  \n",
       "44444          253         253  \n",
       "44445          216         225  \n",
       "\n",
       "[44446 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_encoder = LabelEncoder()\n",
    "genders = np.unique(data['gender'])\n",
    "gender_encoder.fit(genders)\n",
    "gender_encoder.transform(['Men'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['age', 'tripletsOfMonths', 'commonRed', 'commonGreen' ,'commonBlue']]\n",
    "Y = train['gender'].values\n",
    "Y_transform = gender_encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2515066653966933\n",
      "{'estimator': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=10,\n",
      "                      max_features=None, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=None, splitter='best'), 'estimator__max_depth': 10}\n",
      "1.2790276395667655\n",
      "{'estimator': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=100,\n",
      "                      max_features=None, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=None, splitter='best'), 'estimator__max_depth': 100}\n",
      "0.25231524555451185\n",
      "{'estimator': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=10,\n",
      "                      max_features=None, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=None, splitter='best'), 'estimator__max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "param_grid = [\n",
    "    {\n",
    "        'estimator': [LinearRegression()]\n",
    "    },\n",
    "    {\n",
    "        'estimator':[KNeighborsRegressor()],\n",
    "        'estimator__n_neighbors': [10, 50, 100],\n",
    "    },\n",
    "    {\n",
    "        'estimator': [DecisionTreeRegressor()],\n",
    "        'estimator__max_depth': [5, 10, 100],\n",
    "    }\n",
    "]\n",
    "\n",
    "for metric in [r2_score, mean_squared_error, explained_variance_score]:\n",
    "    pipe = Pipeline(steps=[('estimator', LinearRegression())])\n",
    "    grid = GridSearchCV(pipe, param_grid, scoring=make_scorer(metric))\n",
    "    \n",
    "    grid.fit(X,Y_transform)\n",
    "    \n",
    "    print(grid.best_score_)\n",
    "    print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Looks like DecisionTreeRegressor of max depth 10 performed the best out of the three compared. For scores, R^2 and Explained Variance Score are almost the same with Explained Variance having a little bit higher. This could be because it uses biased variance. \n",
    "\n",
    "Mean squared Error on the other hand had a value of ~1.28 which isn't too bad compared to the scores of the other metrics in respect of whats considered 'good'. \n",
    "\n",
    "Since DecisionTreeRegressor performed the best, I will try to improve those scores using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'max_features': 4, 'min_samples_leaf': 75, 'min_samples_split': 2}\n",
      "0.26574687836630156\n",
      "{'max_depth': 1, 'max_features': 1, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "1.1137710980707893\n",
      "{'max_depth': 100, 'max_features': 4, 'min_samples_leaf': 100, 'min_samples_split': 2}\n",
      "0.2666691875474271\n"
     ]
    }
   ],
   "source": [
    "tree_param_grid = {\n",
    "    'max_depth': [1, 5, 10, 50, 100, 300],\n",
    "    'max_features': list(range(1,len(X.columns))),\n",
    "    'min_samples_leaf': [5, 10, 50, 75, 100, 300],\n",
    "    'min_samples_split': list(range(2,5)),\n",
    "}\n",
    "\n",
    "estimators = []\n",
    "\n",
    "for metric in [r2_score, mean_squared_error, explained_variance_score]:\n",
    "    grid_tree = GridSearchCV(DecisionTreeRegressor(), tree_param_grid, scoring=make_scorer(metric))\n",
    "    \n",
    "    grid_tree.fit(X, Y_transform)\n",
    "    estimators.append(('DecisionTree', grid_tree.best_estimator_))\n",
    "    print(grid_tree.best_params_)\n",
    "    print(grid_tree.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I managed to improve improve the R^2 by around ~.015, the Explained Variance by ~.013 and the MSE by ~.16, so not that much overall.\n",
    "\n",
    "It is interesting that the best parameters do not mach for each metric. \n",
    "\n",
    "For R^2, a tree of depth 10, Max Features (the number of variables allowed to use to predict values) of 4, min sample leaf (the lowest its allowed to go for a leaf) of 75, and min samples split (min samples required to split an undetermined node) of 4. \n",
    "\n",
    "Explained Variance had almost the same parameters except for a max depth of 300 and a min sample split of 2. The max depth is interesting since it is really far off from the R^2's best params. \n",
    "\n",
    "MSE had the simpliest parameters of 1 for max depth and max features, 3 for min sample leaf, and 4 for min samples split. The theory behind the smaller values could be the graph was taking into account too much noise. As I learned from doing PCA, most of the data lives in 1 dimension making a single cut pretty effective. \n",
    "\n",
    "Will try to improve some more with an ensomble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2797269738130592\n",
      "{'estimator': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=300, max_features=4, max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=50,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=1000, n_jobs=-1, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False), 'estimator__max_depth': 300, 'estimator__max_features': 4, 'estimator__min_samples_leaf': 50, 'estimator__n_estimators': 1000, 'estimator__n_jobs': -1}\n",
      "0.9012617713983566\n",
      "{'estimator': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=5, max_features=2, max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=300,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=50, n_jobs=-1, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False), 'estimator__max_depth': 5, 'estimator__max_features': 2, 'estimator__min_samples_leaf': 300, 'estimator__n_estimators': 50, 'estimator__n_jobs': -1}\n",
      "0.2797615008321811\n",
      "{'estimator': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=100, max_features=4, max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=50,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=1000, n_jobs=-1, oob_score=False,\n",
      "                      random_state=None, verbose=0, warm_start=False), 'estimator__max_depth': 100, 'estimator__max_features': 4, 'estimator__min_samples_leaf': 50, 'estimator__n_estimators': 1000, 'estimator__n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "ensemble_params = [\n",
    "    {\n",
    "        # bagger\n",
    "        'estimator': [RandomForestRegressor()],\n",
    "        'estimator__n_estimators': [50, 100, 300, 1000],\n",
    "        'estimator__max_depth': [5, 50, 100, 300],\n",
    "        'estimator__max_features': [2, 3, 4],\n",
    "        'estimator__min_samples_leaf': [50, 100, 300],\n",
    "        'estimator__n_jobs':[-1]\n",
    "    }\n",
    " #   {\n",
    "        # takes wayyyyy to long to run all of them, just test the bagger \n",
    "#         # booster\n",
    "#         'estimator': [GradientBoostingRegressor()],\n",
    "#         'estimator__learning_rate': [.01, .001],\n",
    "#         'estimator__n_estimators': [50, 100, 300, 1000],\n",
    "#         'estimator__min_samples_leaf': [50, 100, 300],\n",
    "#         'estimator__max_depth': [5, 50, 100, 300],\n",
    "#         'estimator__max_features': [2, 3, 4]\n",
    "#     },\n",
    "#     {\n",
    "#         # Voter\n",
    "#         'estimator':[VotingRegressor(estimators=estimators)],\n",
    "#         'estimator__n_jobs': [-1]\n",
    "#     }\n",
    "]\n",
    "for metric in [r2_score, mean_squared_error, explained_variance_score]:\n",
    "    pipe = Pipeline(steps=[('estimator', RandomForestRegressor())])\n",
    "    \n",
    "    grid_ensemble = GridSearchCV(pipe, ensemble_params, scoring=make_scorer(metric))\n",
    "    \n",
    "    #sorry computer for what im about to do to you\n",
    "    grid_ensemble.fit(X, Y_transform)\n",
    "    \n",
    "    print(grid_ensemble.best_score_)\n",
    "    print(grid_ensemble.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "    \n",
    "Looks like I was able, to no surprise, increase the scores a little more using an Ensemble method like RandomForest\n",
    "\n",
    "R^2 and Explined Varince are almost exactly the same which is suppose to happen as MSE gets smaller, which it has. \n",
    "\n",
    "MSE got to ~.9 which is interesting but could be misleading. For example, Since the data is is so dense in a small space, as I confirmed using PCA, its easy to make a relative guess as to what the value is suppose to be but being accurate can be really hard. This is why the value is relatively good but will probably not get much better. \n",
    "\n",
    "It is worth noting that the max depth parameter, for the best estimator, for each metric, is almsot completly opposite as to what it was for a single DecisionTree. I would expect it to be lower as more models are being used. It may be because the model is overfitting. For R^2, 1000 estimators of depth 300, seems like a lot. But then again, there is a lot of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
